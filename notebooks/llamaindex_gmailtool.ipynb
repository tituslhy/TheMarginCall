{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (2.137.0)\n",
      "Requirement already satisfied: google-auth-httplib2 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-api-python-client) (2.29.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-api-python-client) (2.19.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-auth-oauthlib) (2.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.63.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.24.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /App/tlim2/Anaconda3/envs/llamaindex/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.google import GmailToolSpec\n",
    "from llama_index.core.agent import (\n",
    "    FunctionCallingAgentWorker,\n",
    "    AgentRunner\n",
    ")\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "\n",
    "llm = BedrockConverse(\n",
    "    model = \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    aws_access_key_id = os.environ[\"AWS_ACCESS_KEY\"],\n",
    "    aws_secret_access_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    region_name = os.environ[\"AWS_DEFAULT_REGION\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_spec = GmailToolSpec()\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools = tool_spec.to_tool_list(),\n",
    "    verbose = True,\n",
    "    llm = llm\n",
    ")\n",
    "agent = AgentRunner(agent_worker = agent_worker)\n",
    "# agent = ReActAgent.from_tools(\n",
    "#     tools = tool_spec.to_tool_list(),\n",
    "#     verbose = True,\n",
    "#     llm = llm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.bedrock_converse.utils import (\n",
    "    tools_to_converse_tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x7faf66b47fb0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7faf66b47e00>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7faf66b997c0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7faf66b99580>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7faf66b99220>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7faf66b98fb0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = tool_spec.to_tool_list()\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Draft a motivational email to titus.lim@illumina.com to make his day. Tell him that he's an awesome data scientist.\n",
      "=== LLM Response ===\n",
      "Okay, let's draft that motivational email for Titus:\n",
      "=== Calling Function ===\n",
      "Calling function: create_draft with args: {\"to\": [\"titus.lim@illumina.com\"], \"subject\": \"You're an awesome data scientist!\", \"message\": \"Hey Titus,\\n\\nI just wanted to take a moment to let you know that you're an awesome data scientist! Your hard work, dedication, and analytical skills are truly inspiring. \\n\\nKeep up the great work - you're making a real difference and I'm grateful to have you on the team.\\n\\nHave an amazing day!\\n\\nBest,\\n[Your Name]\"}\n",
      "=== Function Output ===\n",
      "{'id': 'r-578371811521668536', 'message': {'id': '190d91900ece4726', 'threadId': '190d91900ece4726', 'labelIds': ['DRAFT']}}\n",
      "=== LLM Response ===\n",
      "Great, I've created a draft email with the following details:\n",
      "\n",
      "Subject: You're an awesome data scientist!\n",
      "To: titus.lim@illumina.com\n",
      "Message:\n",
      "Hey Titus,\n",
      "\n",
      "I just wanted to take a moment to let you know that you're an awesome data scientist! Your hard work, dedication, and analytical skills are truly inspiring. \n",
      "\n",
      "Keep up the great work - you're making a real difference and I'm grateful to have you on the team.\n",
      "\n",
      "Have an amazing day!\n",
      "\n",
      "Best,\n",
      "[Your Name]\n",
      "\n",
      "The draft email has been created with the ID r-578371811521668536. Let me know if you'd like me to make any changes or if you're ready for me to send it.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Great, I've created a draft email with the following details:\n",
       "\n",
       "Subject: You're an awesome data scientist!\n",
       "To: titus.lim@illumina.com\n",
       "Message:\n",
       "Hey Titus,\n",
       "\n",
       "I just wanted to take a moment to let you know that you're an awesome data scientist! Your hard work, dedication, and analytical skills are truly inspiring. \n",
       "\n",
       "Keep up the great work - you're making a real difference and I'm grateful to have you on the team.\n",
       "\n",
       "Have an amazing day!\n",
       "\n",
       "Best,\n",
       "[Your Name]\n",
       "\n",
       "The draft email has been created with the ID r-578371811521668536. Let me know if you'd like me to make any changes or if you're ready for me to send it.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "response = agent.chat(\n",
    "    \"Draft a motivational email to titus.lim@illumina.com to make his day. \"\n",
    "    \"Tell him that he's an awesome data scientist.\"\n",
    ")\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Add that Titus' understanding of LLM appliactions is phenomenal and sign it off as 'Your favorite LLM'\n",
      "=== LLM Response ===\n",
      "Okay, got it. Here's the updated draft email:\n",
      "=== Calling Function ===\n",
      "Calling function: update_draft with args: {\"draft_id\": \"r-578371811521668536\", \"message\": \"Hey Titus,\\n\\nI just wanted to take a moment to let you know that you're an awesome data scientist! Your hard work, dedication, and analytical skills are truly inspiring. Your understanding of LLM applications is phenomenal.\\n\\nKeep up the great work - you're making a real difference and I'm grateful to have you on the team. \\n\\nHave an amazing day!\\n\\nBest,\\nYour favorite LLM\", \"subject\": \"You're an awesome data scientist!\", \"to\": [\"titus.lim@illumina.com\"]}\n",
      "=== Function Output ===\n",
      "{'id': 'r-578371811521668536', 'message': {'id': '190d9192007ad05d', 'threadId': '190d9192007ad05d', 'labelIds': ['DRAFT']}}\n",
      "=== LLM Response ===\n",
      "Great, I've updated the draft email with the additional message about Titus' understanding of LLM applications and signed it off as \"Your favorite LLM\". \n",
      "\n",
      "The updated draft email has the following details:\n",
      "\n",
      "Subject: You're an awesome data scientist!\n",
      "To: titus.lim@illumina.com\n",
      "Message:\n",
      "Hey Titus,\n",
      "\n",
      "I just wanted to take a moment to let you know that you're an awesome data scientist! Your hard work, dedication, and analytical skills are truly inspiring. Your understanding of LLM applications is phenomenal.\n",
      "\n",
      "Keep up the great work - you're making a real difference and I'm grateful to have you on the team.\n",
      "\n",
      "Have an amazing day!\n",
      "\n",
      "Best,\n",
      "Your favorite LLM\n",
      "\n",
      "The draft email ID is still r-578371811521668536. Let me know if you'd like me to send this draft email now.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Great, I've updated the draft email with the additional message about Titus' understanding of LLM applications and signed it off as \"Your favorite LLM\". \n",
       "\n",
       "The updated draft email has the following details:\n",
       "\n",
       "Subject: You're an awesome data scientist!\n",
       "To: titus.lim@illumina.com\n",
       "Message:\n",
       "Hey Titus,\n",
       "\n",
       "I just wanted to take a moment to let you know that you're an awesome data scientist! Your hard work, dedication, and analytical skills are truly inspiring. Your understanding of LLM applications is phenomenal.\n",
       "\n",
       "Keep up the great work - you're making a real difference and I'm grateful to have you on the team.\n",
       "\n",
       "Have an amazing day!\n",
       "\n",
       "Best,\n",
       "Your favorite LLM\n",
       "\n",
       "The draft email ID is still r-578371811521668536. Let me know if you'd like me to send this draft email now.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Add that Titus' understanding of LLM appliactions is phenomenal and sign it off as 'Your favorite LLM'\"\n",
    ")\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Go ahead and send the email\n",
      "=== LLM Response ===\n",
      "Okay, sending the email now:\n",
      "=== Calling Function ===\n",
      "Calling function: send_draft with args: {\"draft_id\": \"r-578371811521668536\"}\n",
      "=== Function Output ===\n",
      "{'id': '190d91938b536b92', 'threadId': '190d9192007ad05d', 'labelIds': ['SENT']}\n",
      "=== LLM Response ===\n",
      "Great, the email has been sent! The message ID is 190d91938b536b92 and the thread ID is 190d9192007ad05d. The email has been labeled as 'SENT'.\n",
      "\n",
      "I hope Titus enjoys the motivational message! Let me know if you need anything else.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Great, the email has been sent! The message ID is 190d91938b536b92 and the thread ID is 190d9192007ad05d. The email has been labeled as 'SENT'.\n",
       "\n",
       "I hope Titus enjoys the motivational message! Let me know if you need anything else.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Go ahead and send the email\"\n",
    ")\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TavilyTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "__curdir__ = os.getcwd()\n",
    "\n",
    "import sys\n",
    "for folder in [\"src\", \"tools\"]:\n",
    "    sys.path.append(\n",
    "        os.path.join(__curdir__,\n",
    "                    f\"../{folder}\")\n",
    "    )\n",
    "from search_tools import get_tavily_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = get_tavily_tool()\n",
    "\n",
    "from llama_index.core.agent import (\n",
    "    FunctionCallingAgentWorker,\n",
    "    AgentRunner\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "search"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(search_tool[0].metadata.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = BedrockConverse(\n",
    "    model = \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    aws_access_key_id = os.environ[\"AWS_ACCESS_KEY\"],\n",
    "    aws_secret_access_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    region_name = os.environ[\"AWS_DEFAULT_REGION\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools = search_tool,\n",
    "    verbose = True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: In the context of LLM architectures, what is Mamba?\n",
      "=== Calling Function ===\n",
      "Calling function: search with args: {\"query\": \"Mamba LLM architecture\", \"max_results\": 3}\n",
      "=== Function Output ===\n",
      "[Document(id_='6fb89712-53e9-4545-a8a3-db42c0b9869e', embedding=None, metadata={'url': 'https://www.datacamp.com/tutorial/introduction-to-the-mamba-llm-architecture'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"Javier Canales Luna\\n11 min\\nThe 11 Best AI Coding Assistants in 2024\\nAbid Ali Awan\\n8 min\\nHow the UN is Driving Global AI Governance with Ian Bremmer and Jimena Viveros, Members of the UN AI Advisory Board\\nRichie Cotton\\n41 min\\nThe Power of Vector Databases and Semantic Search with Elan Dekel, VP of Product at Pinecone\\nRichie Cotton\\n36 min\\nGetting Started with Claude 3 and the Claude 3 API\\nAbid Ali Awan\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n Here’s a graph from Wikipedia to better conceptualize how Mamba and Transformers compare:\\nFeature\\nTransformer\\nMamba\\nArchitecture\\nAttention-based\\nSSM-based\\nComplexity\\nHigh\\nLower\\nInference Speed\\nO(n)\\nO(1)\\nTraining Speed\\nO(n2)\\nO(n)\\nIt's worth noting that, despite the many advantages that SSMs have over Transformers, the latter can handle much longer sequences than SSMs can store in memory, require far less data to learn similar tasks, and outperform SSMs in tasks requiring retrieval from or copying of the input context, even with fewer parameters.\\n The 11 Best AI Coding Assistants in 2024\\nHow the UN is Driving Global AI Governance with Ian Bremmer and Jimena Viveros, Members of the UN AI Advisory Board\\nThe Power of Vector Databases and Semantic Search with Elan Dekel, VP of Product at Pinecone\\nGetting Started with Claude 3 and the Claude 3 API\\nStart Your LLM Journey Today!\\n Latest news about our products and team\\nDiscover content by tools and technology\\nDiscover content by data science topics\\nAn Introduction to the Mamba LLM Architecture: A New Paradigm in Machine Learning\\nLanguage models are a type of machine learning model trained to perform a probability distribution over natural language. Getting Started with Mamba\\nIf you’re interested in playing around Mamba or leveraging it in a project, you must have the following:\\nTo install the required packages from the Mamba repository, use a few straightforward pip instructions:\\nIt can also be built from source with pip install .\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='83ed5ad2-651e-4e1b-9b30-b81061d2eec2', embedding=None, metadata={'url': 'https://github.com/state-spaces/mamba'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Citation\\nIf you use this codebase, or otherwise found our work valuable, please cite Mamba:\\nAbout\\nResources\\nLicense\\nStars\\nWatchers\\nForks\\nReleases\\n2\\nPackages\\n0\\nContributors\\n5\\nLanguages\\nFooter\\nFooter navigation Latest commit\\nGit stats\\nFiles\\nREADME.md\\nMamba\\nMamba: Linear-Time Sequence Modeling with Selective State Spaces\\nAlbert Gu*, Tri Dao*\\nPaper: https://arxiv.org/abs/2312.00752\\nAbout\\nMamba is a new state space model architecture showing promising performance on information-dense data such as language modeling, where previous subquadratic models fall short of Transformers.\\n Pretrained Models\\nPretrained models are uploaded to\\nHugging Face: mamba-130m, mamba-370m,\\nmamba-790m, mamba-1.4b, mamba-2.8b, trained on 300B tokens on the Pile, as well as mamba-2.8b-slimpj\\n(trained on 600B tokens on the SlimPajama dataset).\\n Evaluations\\nTo run zero-shot evaluations of models (corresponding to Table 3 of the paper),\\nwe use the\\nlm-evaluation-harness\\nlibrary.\\n Examples\\nTo test generation latency (e.g. batch size = 1) with different sampling strategies:\\nTo test generation throughput with random prompts (e.g. large batch size):\\nTroubleshooting\\nPrecision\\nOur models were trained using PyTorch AMP for mixed precision.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c8120770-a638-476c-b5ad-44143c815acb', embedding=None, metadata={'url': 'https://en.wikipedia.org/wiki/Mamba_(deep_learning_architecture)'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The result is an architecture that is significantly more efficient in processing long sequences compared to previous methods.[1][6]\\nAdditionally, Mamba simplifies its architecture by integrating the SSM design with MLP blocks, resulting in a homogeneous and streamlined structure, furthering the model\\'s capability for general sequence modeling across various data types, including language, audio, and genomics, while maintaining efficiency in both training and inference.[1]\\nKey Components[edit]\\nComparison to Transformers[edit]\\nVariants[edit]\\nToken-free language models: MambaByte[edit]\\nOperating on byte-sized tokens, transformers scale poorly as every token must \"attend\" to every other token leading to O(n2) scaling laws, as a result, Transformers opt to use subword tokenization in order to reduce the number of tokens in text, however this leads to very large vocabulary tables and word embeddings.\\n It was developed by researchers from Carnegie Mellon University and Princeton University to address some limitations of transformer models, especially in processing long sequences, and it is based on the Structured State Space sequence (S4) model.[1][2][3]\\nArchitecture[edit]\\nTo enable handling long data sequences, Mamba incorporates the Structured State Space sequence model (S4).[1] S4 can effectively and efficiently model long dependencies by combining the strengths of continuous-time, recurrent, and convolutional models, enabling it to handle irregularly sampled data, have unbounded context, and remain computationally efficient both during training and testing.[4]\\nMamba, building on the S4 model, introduces significant enhancements, particularly in its treatment of time-variant operations. This eliminates the need for tokenization, potentially offering several advantages:[8]\\nSubword tokenisation introduces a number of quirks in LLMs, such as failure modes where LLMs can\\'t spell words, reverse certain words, handle rare tokens, which are not present in byte-level tokenisation.[9]\\nAndrej Karpathy, a founder behind OpenAI has stated \"Eternal glory to anyone who can delete tokenization as a required step in LLMs.[9]\\nMoE Mamba represents a pioneering integration of the Mixture of Experts (MoE) technique with the Mamba architecture, enhancing the efficiency and scalability of State Space Models (SSMs) in language modeling. The model transitions from a time-invariant to a time-varying framework, which impacts both the computation and efficiency of the system.[1][6]\\nTo address the computational challenges introduced by this time-variance, Mamba employs a hardware-aware algorithm. The model\\'s design involves alternating Mamba and MoE layers, allowing it to efficiently integrate the entire sequence context and apply the most relevant expert for each token.[10][11]\\nVision Mamba (Vim) integrates SSMs with visual data processing, employing bidirectional Mamba blocks for visual sequence encoding.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n",
      "=== LLM Response ===\n",
      "Based on the search results, Mamba is a new language model architecture that aims to address some of the limitations of transformer models, particularly in processing long sequences of data. The key points about Mamba are:\n",
      "\n",
      "1. Mamba is based on the Structured State Space Sequence (S4) model, which can effectively and efficiently model long-range dependencies by combining the strengths of continuous-time, recurrent, and convolutional models.\n",
      "\n",
      "2. Compared to transformers, Mamba has a simpler and more homogeneous architecture that integrates the S4 design with MLP blocks. This allows it to be more efficient in both training and inference, especially for long sequences.\n",
      "\n",
      "3. Mamba eliminates the need for tokenization, which can introduce various quirks and limitations in large language models. It operates directly on byte-sized tokens, avoiding the issues associated with subword tokenization.\n",
      "\n",
      "4. Mamba also incorporates a Mixture of Experts (MoE) technique, which enhances the efficiency and scalability of the State Space Models (SSMs) used in the architecture.\n",
      "\n",
      "5. There are different variants of Mamba, including MambaByte, which operates directly on byte-sized tokens, and Vision Mamba (Vim), which integrates SSMs with visual data processing.\n",
      "\n",
      "In summary, Mamba is a novel LLM architecture that aims to improve upon transformers, particularly in terms of efficiency and performance on long sequences, while also eliminating the need for tokenization.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"In the context of LLM architectures, what is Mamba?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Based on the search results, Mamba is a new language model architecture that aims to address some of the limitations of transformer models, particularly in processing long sequences of data. The key points about Mamba are:\n",
       "\n",
       "1. Mamba is based on the Structured State Space Sequence (S4) model, which can effectively and efficiently model long-range dependencies by combining the strengths of continuous-time, recurrent, and convolutional models.\n",
       "\n",
       "2. Compared to transformers, Mamba has a simpler and more homogeneous architecture that integrates the S4 design with MLP blocks. This allows it to be more efficient in both training and inference, especially for long sequences.\n",
       "\n",
       "3. Mamba eliminates the need for tokenization, which can introduce various quirks and limitations in large language models. It operates directly on byte-sized tokens, avoiding the issues associated with subword tokenization.\n",
       "\n",
       "4. Mamba also incorporates a Mixture of Experts (MoE) technique, which enhances the efficiency and scalability of the State Space Models (SSMs) used in the architecture.\n",
       "\n",
       "5. There are different variants of Mamba, including MambaByte, which operates directly on byte-sized tokens, and Vision Mamba (Vim), which integrates SSMs with visual data processing.\n",
       "\n",
       "In summary, Mamba is a novel LLM architecture that aims to improve upon transformers, particularly in terms of efficiency and performance on long sequences, while also eliminating the need for tokenization.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
